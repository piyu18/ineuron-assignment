{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Corpora?\n",
        "2. What are Tokens?\n",
        "3. What are Unigrams, Bigrams, Trigrams?\n",
        "4. How to generate n-grams from text?\n",
        "5. Explain Lemmatization\n",
        "6. Explain Stemming\n",
        "7. Explain Part-of-speech (POS) tagging\n",
        "8. Explain Chunking or shallow parsing\n",
        "9. Explain Noun Phrase (NP) chunking\n",
        "10. Explain Named Entity Recognition"
      ],
      "metadata": {
        "id": "GNiPJqtSi5pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Corpora?\n",
        "**Corpora:** In natural language processing (NLP), a corpus (plural: \"corpora\") is a large collection of text data that is used to train and evaluate NLP models. Corpora are typically used to develop and test language models, speech recognition systems, and other NLP applications.\n",
        "Corpora are a very important resource for training and evaluating NLP models, as they provide the model with a large amount of text data that is representative of the target domain or task."
      ],
      "metadata": {
        "id": "j8E0RpxpjXx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are Tokens?\n",
        "In natural language processing (NLP), a token is a sequence of characters that represents a single unit of meaning in a text. Tokens are the basic building blocks of text data and are used in various NLP tasks such as text classification, language translation, and language generation. The process of breaking a text into tokens is called tokenization.\n",
        "\n",
        "There are several types of tokens that can be used in NLP, including:\n",
        "\n",
        "*   Words\n",
        "*   Sentences\n",
        "*   Subwords\n",
        "*   Characters\n",
        "*   N-grams\n",
        "\n",
        "The choice of tokenization depends on the specific NLP task and the language being studied. For example, word-level tokenization is often used for tasks such as text classification, while character-level tokenization is used for tasks such as text generation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gS8xDkv_jxoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are Unigram, Bigram and Trigram\n",
        "In natural language processing (NLP), unigrams, bigrams, and trigrams are types of tokens that represent sequences of words in text data.\n",
        "\n",
        "**Unigrams:** An unigram is a single word token. For example, in the sentence \"I love to play soccer\", the unigrams would be [\"I\", \"love\", \"to\", \"play\", \"soccer\"].\n",
        "\n",
        "**Bigrams:** A bigram is a sequence of two words. For example, in the sentence \"I love to play soccer\", the bigrams would be [\"I love\", \"love to\", \"to play\", \"play soccer\"].\n",
        "\n",
        "**Trigrams:** A trigram is a sequence of three words. For example, in the sentence \"I love to play soccer\", the trigrams would be [\"I love to\", \"love to play\", \"to play soccer\"].\n",
        "\n",
        "These type of tokens are used in various NLP tasks such as text classification, language translation and language generation. They are often used as features in NLP models.\n",
        "\n",
        "For example, if you want to predict the next word in a sentence given the previous two words, using trigrams as features would be more informative than using unigrams. Similarly, in sentiment analysis, using bigrams can provide more information as compared to using unigrams as they can capture the sentiment conveyed by a combination of words.\n",
        "\n",
        "In summary, Unigrams, Bigrams and Trigrams are useful in capturing the context of words and providing more information to the model than just individual words"
      ],
      "metadata": {
        "id": "Meprqfurk049"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How to generate n-grams from text?"
      ],
      "metadata": {
        "id": "pabG-nVQltgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "text = \"I love to play hockey\"\n",
        "tokens = text.split()\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = ngrams(tokens, 2)\n",
        "print(list(bigrams))\n",
        "\n",
        "# Generate trigrams\n",
        "trigrams = ngrams(tokens, 3)\n",
        "print(list(trigrams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laCDE-vbm6Wz",
        "outputId": "3382118d-e079-4103-9f2f-467ab7207b4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'love'), ('love', 'to'), ('to', 'play'), ('play', 'hockey')]\n",
            "[('I', 'love', 'to'), ('love', 'to', 'play'), ('to', 'play', 'hockey')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain Lemmatization"
      ],
      "metadata": {
        "id": "QyVEGHiVm87q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a process of reducing a word to its meaningful base or root form. It is a more sophisticated form of stemming, which simply cuts off the end of a word to reduce it to its root form. "
      ],
      "metadata": {
        "id": "H29ShSEtGH4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gazdg8ONLCsn",
        "outputId": "aa72f995-5020-4fd5-bc05-4aa238c55924"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"ran\", \"runner\", \"run\", \"caring\", \"books\"]\n",
        "lemma_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemma_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhwpUcANKk8y",
        "outputId": "f3f5f282-e435-48ce-c8cf-880933294bc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'ran', 'runner', 'run', 'caring', 'book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain Stemming"
      ],
      "metadata": {
        "id": "l3gZXnl8nXIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a technique used to extract the base form of the words by removing affixes from them."
      ],
      "metadata": {
        "id": "YobO82Y5Lv0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"ran\", \"runner\", \"run\", \"caring\", \"books\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2FthgUQHKID",
        "outputId": "3ed276ae-8131-4d14-f589-dc609b75d9f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'ran', 'runner', 'run', 'care', 'book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagging\n",
        "Part-of-speech (POS) tagging is the process of assigning a grammatical category to each word in a text based on its context. This is also known as grammatical tagging or word-category disambiguation. The goal of POS tagging is to identify the syntactic role of each word in a sentence, such as noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc."
      ],
      "metadata": {
        "id": "IffVZ8-2o3dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required POS tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the sentence\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag the tokenized sentence\n",
        "tagged_sentence = nltk.pos_tag(tokens)\n",
        "\n",
        "print(tagged_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXqX0kRso2VQ",
        "outputId": "89bc8282-c8cb-487e-d474-7e79d539bf54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking, also known as shallow parsing, is a natural language processing technique that involves identifying and extracting phrases from unstructured text. The goal of chunking is to group words together into meaningful units, such as noun phrases, verb phrases, and prepositional phrases.\n",
        "\n",
        "In chunking, the words in a sentence are tagged with their corresponding POS tags, and then grouped together into chunks based on a set of predefined rules. The resulting chunks are typically represented in a tree structure, with the chunks as the internal nodes and the individual words as the leaves.\n",
        "\n"
      ],
      "metadata": {
        "id": "BQPUvsp-pA8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Define the sentence\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag the tokenized sentence\n",
        "tagged_sentence = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define the grammar for chunking\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Create a parser\n",
        "parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "# Perform chunking\n",
        "chunked_sentence = parser.parse(tagged_sentence)\n",
        "\n",
        "print(chunked_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7xoLdpyqAjA",
        "outputId": "2887e31d-8b61-43df-b4a2-e56815abc735"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP The/DT cat/NN) sat/VBD on/IN (NP the/DT mat/NN) ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explain Noun Phrase (NP) chunking"
      ],
      "metadata": {
        "id": "912mxdtmqZBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun Phrase (NP) chunking is a specific form of shallow parsing, which is used to extract noun phrases from unstructured text. The goal of NP chunking is to identify and group together words in a sentence that form a noun phrase.\n",
        "\n",
        "In NP chunking, the words in a sentence are first tagged with their corresponding POS tags, and then grouped together into NP chunks based on a set of predefined rules. The resulting chunks are typically represented in a tree structure, with the chunks as the internal nodes and the individual words as the leaves.\n",
        "\n"
      ],
      "metadata": {
        "id": "2luuR0Rjqj3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NER"
      ],
      "metadata": {
        "id": "TK9yVm3nqCIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER) is a subtask of natural language processing that involves identifying and extracting specific information from unstructured text. The goal of NER is to locate and classify named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
      ],
      "metadata": {
        "id": "8NHa_fu7qUU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oN8DoknrVdM",
        "outputId": "358ccc77-4fff-468a-b840-53b98a2ef1a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Define the sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and served as the 44th President of the United States\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tag the tokenized sentence\n",
        "tagged_sentence = nltk.pos_tag(tokens)\n",
        "\n",
        "# Perform NER\n",
        "ner_output = nltk.ne_chunk(tagged_sentence)\n",
        "\n",
        "print(ner_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3_BYZM4rDUQ",
        "outputId": "83a6a292-4eb0-4577-c054-c06b6c6995f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Hawaii/NNP)\n",
            "  and/CC\n",
            "  served/VBD\n",
            "  as/IN\n",
            "  the/DT\n",
            "  44th/CD\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoJ9wJ_OrSYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}